import pandas as pd
import os

# === CONFIGURATION ===
# Ensure these paths point to the correct files.
BASE_FOLDER = r"D:\Projects\BPGscript\downloaded_pdfs"

# --- INPUT FILES ---
# The duplicate map generated by the first script.
duplicate_map_path = os.path.join(BASE_FOLDER, "duplicate_map_grouped.xlsx")
# The Excel file containing your original list of filenames and links.
referral_sheet_path = os.path.join(BASE_FOLDER, "referral_links_first500.xlsx") 

# --- OUTPUT FILE ---
# The final, combined report will be saved here.
consolidated_output_path = os.path.join(BASE_FOLDER, "consolidated_duplicate_report.xlsx")

# === Main Logic ===

print("Starting the consolidation process...")

# --- Step 1: Load the input files into pandas DataFrames ---
try:
    print(f"Loading duplicate map from: {duplicate_map_path}")
    df_duplicates = pd.read_excel(duplicate_map_path)

    print(f"Loading referral links from: {referral_sheet_path}")
    df_links = pd.read_excel(referral_sheet_path)
    # Standardize column names for easier use
    df_links.rename(columns={"File Name": "FileName", "PDF LINK": "PDFLink"}, inplace=True)

except FileNotFoundError as e:
    print(f"\n--- ERROR ---")
    print(f"Could not find a required file: {e.filename}")
    print("Please make sure both the duplicate map and the referral sheet exist in the correct locations.")
    exit() # Stop the script if files are missing

# --- Step 2: Create fast lookup dictionaries for efficient processing ---

# Map every duplicate file to its original file.
# e.g., {'pdf_121.pdf': 'pdf_112.pdf', 'pdf_135.pdf': 'pdf_112.pdf', ...}
duplicate_to_original_map = {}
for _, row in df_duplicates.iterrows():
    original_file = row['Original File']
    # The 'Duplicate Files (Moved)' column is a string, so we split it into a list
    duplicates_list = str(row['Duplicate Files (Moved)']).split(', ')
    for dup_file in duplicates_list:
        if dup_file: # Ensure not an empty string
            duplicate_to_original_map[dup_file] = original_file

# Map every original file to its duplicate count
# e.g., {'pdf_112.pdf': 20, 'pdf_136.pdf': 17, ...}
original_to_count_map = pd.Series(df_duplicates['Duplicate Count'].values, index=df_duplicates['Original File']).to_dict()

# Map every filename (from the referral sheet) to its link
# e.g., {'pdf_1.pdf': 'http://...', 'pdf_2.pdf': 'http://...', ...}
filename_to_link_map = pd.Series(df_links.PDFLink.values, index=df_links.FileName).to_dict()

# --- Step 3: Process every file from the referral sheet to build the report ---

print("Processing and consolidating data...")
report_data = []

# Iterate through every file listed in your original referral sheet
for filename in df_links['FileName']:
    
    file_status = "Original"
    original_file_mapping = filename  # By default, a file is its own original

    # Check if this file was identified as a duplicate
    if filename in duplicate_to_original_map:
        file_status = "Duplicate"
        original_file_mapping = duplicate_to_original_map[filename]

    # Get the details for the original file it maps to
    duplicate_count = original_to_count_map.get(original_file_mapping, 0) # Get count, default to 0
    original_file_link = filename_to_link_map.get(original_file_mapping, "Link Not Found")

    report_data.append({
        "File Name": filename,
        "PDF Link": filename_to_link_map.get(filename, "Link Not Found"),
        "Status": file_status,
        "Maps to Original File": original_file_mapping,
        "Original File Link": original_file_link,
        "Total Duplicates for Original": duplicate_count
    })

# --- Step 4: Create the final DataFrame, sort it, and save to Excel ---

if not report_data:
    print("No data was processed. Exiting.")
    exit()

# Create the final DataFrame from our list of processed data
df_final_report = pd.DataFrame(report_data)

# Sort the report by filename in a natural, numerical order
# Extracts the number from 'pdf_123.pdf' to sort correctly
df_final_report['sort_key'] = df_final_report['File Name'].str.extract('(\d+)').astype(int)
df_final_report.sort_values(by='sort_key', ascending=True, inplace=True)
df_final_report.drop(columns='sort_key', inplace=True) # Remove the temporary sort key

# Save the final consolidated report to an Excel file
df_final_report.to_excel(consolidated_output_path, index=False)

print("\n--- CONSOLIDATION COMPLETE ---")
print(f"âœ… Successfully created the consolidated report!")
print(f"   Saved to: {consolidated_output_path}")
print("---------------------------------")